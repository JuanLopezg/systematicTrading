# -*- coding: utf-8 -*-
"""Building a Survivorship Bias-Free Cryptocurrency Dataset - [Full Version]

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oc_53xSSLb0x2U6hQl8Anvo8SXkjaXqO

# **CryptoDB: Building a Survivorship Bias-Free Cryptocurrency Dataset - [Full Version]**

**Author:**  
[**Mohamed Gabriel**](https://www.linkedin.com/in/msmgabriel/)  
*Software Engineer at Concretum Group*

This notebook provides a **comprehensive pipeline** for fetching, processing, and analyzing the **complete cryptocurrency market data** from the CoinMarketCap API. The focus is on including **both active and inactive (delisted or failed)** coins to avoid **survivorship bias**, which is a common issue in many crypto backtesting workflows.

Originally, this code was written by [**Dr. Andrea Barbon**](https://www.linkedin.com/in/andrea-barbon-89665631/) for the research paper [**"Catching Crypto Trends: A Tactical Approach for Bitcoin and Altcoins"**](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5209907).

> ⚠️ **Warning**: This notebook builds the **full cryptocurrency market dataset** with ~23,286 cryptocurrencies and ~28.6 million daily observations. The complete process takes **approximately 22+ hours** to run.
>
> For a lighter introduction, see the [10-sample version](https://colab.research.google.com/drive/1hYjVDR6G6Ndw-JagarFCdpzDD-q-kcY8?usp=sharing).

Feel free to use, adapt, and extend this code for your own research, trading systems, or academic projects.

## 1. Setup and Imports
"""

import pandas as pd
import numpy as np
import os
import requests
from tqdm import tqdm as tq
from datetime import datetime as dt
from pandas.tseries.offsets import *
from glob import glob as g
import matplotlib.pyplot as plt

from requests import Request, Session
from requests.exceptions import ConnectionError, Timeout, TooManyRedirects
import json
import time
import gc
"""## 2. API Configuration

"""

# CoinMarketCap API Key
KEY = 'key'

# Configure API headers
headers = {
  'Accepts': 'application/json',
  'X-CMC_PRO_API_KEY': KEY,
}

# Base URL for the API
base_url = "https://pro-api.coinmarketcap.com/"

"""## 3. Data Collection Functions

"""

def get_daily_OHLCV(the_id):
    """
    Fetch daily OHLCV (Open, High, Low, Close, Volume) data for a cryptocurrency

    Parameters:
    the_id (int): CoinMarketCap ID of the cryptocurrency

    Returns:
    DataFrame: OHLCV data for the specified cryptocurrency
    """
    time.sleep(0.01)  # We have a limit of 90 calls per minute
    url = base_url + 'v2/cryptocurrency/ohlcv/historical'
    parameters = {
        'id': the_id,
        'time_period': 'hourly',
        'interval': '1h',
        'time_start': '2024-10-1',
        'time_end': '2025-9-21'
    }

    session = Session()
    session.headers.update(headers)

    try:
        response = session.get(url, params=parameters)
        data = json.loads(response.text)
        
        quotes = data['data']['quotes']

        # Build dataframe from USD quote data
        te = pd.DataFrame([q['quote']['USD'] for q in quotes])

        # Use time_open, truncated to the hour (YYYY-MM-DDTHH)
        te['ts'] = pd.to_datetime([q['time_open'] for q in quotes])


        te['id'] = data['data']['id']
        te['symbol'] = data['data']['symbol']
        te = te[['id', 'symbol', 'ts', *te.columns[:6]]]
        return te
    except Exception as e:
        print(e)
        return None

"""### 3.1 Metadata Collection Function

"""

# Define relevant tags for metadata filtering
relevant_tags = [
    'wrapped', 'stablecoin', 'collectibles-nfts', 'memes', 'iot', 'dao',
    'governance', 'mineable', 'pow', 'pos', 'sha-256', 'store-of-value',
    'medium-of-exchange', 'scrypt', 'layer-1', 'layer-2',
]

# Columns to extract from metadata
cols = ['id', 'name', 'symbol', 'category', 'tags']

def get_metadata(the_ids):
    """
    Fetch metadata for a list of cryptocurrency IDs

    Parameters:
    the_ids (list): List of CoinMarketCap IDs

    Returns:
    DataFrame: Metadata for the specified cryptocurrencies
    """
    time.sleep(0.01)  # We have a limit of 90 calls per minute
    url = base_url + 'v2/cryptocurrency/info'
    parameters = {'id': ",".join([str(x) for x in the_ids]), 'skip_invalid': 'true'}
    session = Session()
    session.headers.update(headers)

    try:
        response = session.get(url, params=parameters)
        data = json.loads(response.text)['data']
        the_ids = [x for x in data.keys() if int(x) in the_ids]
        te = pd.DataFrame([data[str(x)] for x in the_ids])[cols]
        te = te.fillna("[0]")
        for tag in relevant_tags:
            te[tag] = te['tags'].apply(lambda x: tag in x)

        return te.drop('tags', axis=1)
    except Exception as e:
        print(e)
        return None

gc.collect()
res = pd.read_parquet("crypto hourly 2025/metadata.par")

ids_to_delete = res[(res['stablecoin'] == True) | (res['wrapped'] == True)]['id'].tolist()

del res
gc.collect()

folders = [ "crypto2H", "crypto4H", "crypto8H", "crypto12H", "cryptoDailyShiftedBy3H" , "cryptoDailyShiftedBy6H" , "cryptoDailyShiftedBy12H"]

for folder in folders:
    files = g(f"{folder}/OHLCV*.par")
    print(f"{len(files)} files")
    dfs = []
    for file in files:
        df = pd.read_parquet(file)
        dfs.append(df)
    df = pd.concat(dfs)

    df.drop(ids_to_delete, inplace=True, errors='ignore')
    gc.collect()

    # Add statistics for each cryptocurrency
    df['stat'] = df.groupby(['id'])['ts'].transform('min')
    df['end'] = df.groupby(['id'])['ts'].transform('max')
    df['hours'] = df.groupby(['id'])['ts'].transform('size')

    print("{0:0,.0f} cryptos".format(df['id'].nunique()))
    print("{0:0,.0f} observations".format(len(df)))




    # Attach metadata to OHLCV data
    df.set_index('id', inplace=True)
    gc.collect()

    df.sort_values(['id', 'ts'], inplace=True)
    gc.collect()

    df.to_parquet(f"{folder}.par")
